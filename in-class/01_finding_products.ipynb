{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f576c4d4-f1e4-443c-b8e0-695b3a25ba8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Finding cloud products!\n",
    "How to use the \"requests\" library in conjunction with the Cloud Service Provider's data to find and open your desired files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0896fdc-ece5-4ea9-9a8e-459e5ec7b5e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89794c5d-076a-4ddb-994c-417b5444fe69",
   "metadata": {
    "tags": []
   },
   "source": [
    "How to find datasets on AWS:     \n",
    "1. Go to the NODD page (https://www.noaa.gov/nodd/datasets), locate your dataset, and click on the \"Amazon Web Services\" link.   \n",
    "2. Under \"Resources on AWS\", locate your desired bucket. Then, click \"Browse Bucket.\"\n",
    "3. Click through your desired product, date, and time, until you see the list of files.\n",
    "4. To the right of the \"AWS S3 Explorer\" text on the top of your screen, you'll see the file path you followed. Adjust the name_string variable in the cell below to reflect your new product path. You can either choose generic variable names, then fill them in with your specific request (as shown below), or copy the specific file path directly into name_string, and delete the generic variable definitions (i.e. name_string = \"noaa-goes18/ABI-L2-ACHAC/2023/003/02\").      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca492bd3-cb87-43a5-8dd3-2d76cfe195a6",
   "metadata": {},
   "source": [
    "**For this tutorial:** GOES (16, 17, 18) buckets: https://registry.opendata.aws/noaa-goes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f1a609-caf9-4652-bdc4-7f91edcb85b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import s3fs\n",
    "import netCDF4\n",
    "\n",
    "# Select your desired variables by looking at the product documentation (see instructions above!)\n",
    "satellite = \"noaa-goes16\"\n",
    "product = \"ABI-L2-LSTM\"\n",
    "year = \"2023\"\n",
    "day_of_year = \"004\"\n",
    "hour = \"17\"\n",
    "\n",
    "name_string = f'{satellite}/{product}/{year}/{day_of_year}/{hour}'\n",
    "\n",
    "# Using name_string as a file path, search for all files that match your criteria.\n",
    "aws = s3fs.S3FileSystem(anon=True)\n",
    "data_files = aws.ls(name_string, refresh=True)\n",
    "\n",
    "# See which files your search produced, and return the file name.\n",
    "fnames = []\n",
    "\n",
    "for f in data_files:\n",
    "    fname = f[len(satellite)+1:]\n",
    "    fnames.append(fname)\n",
    "    print(fname)\n",
    "\n",
    "# Use the requests library to find each file that met your search criteria. \n",
    "for file in fnames:\n",
    "    resp = requests.get(f'https://{satellite}.s3.amazonaws.com/{file}')\n",
    "    \n",
    "    # Read the file in as a netCDF file, and you may begin analysis.\n",
    "    nc = netCDF4.Dataset(file, memory = resp.content)\n",
    "    \n",
    "    # your analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06daf37c-d5fe-419d-a43e-3e225e706cd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c0d7b-e118-4325-8368-f1304f303041",
   "metadata": {},
   "source": [
    "How to find datasets on Azure:     \n",
    "1. Go to the NODD page (https://www.noaa.gov/nodd/datasets), locate your dataset, and click on the \"Microsoft Azure\" link.   \n",
    "2. Scroll down for documentation of product availability and file path. \n",
    "3. Adjust the name_string variable in the cell below to reflect your new product path. You can either choose generic variable names, then fill them in with your specific request (as shown below), or copy the specific file path directly into name_string, and delete the generic variable definitions (i.e. name_string = \"ABI-L2-ACHAC/2023/003/02/\").      \n",
    "4. Make sure to change the container name from the \"satellite\" variable to the name of the container as documented on Azure's dataset page. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfffbd4f-b1a1-404f-8c1d-7cd754b6cd69",
   "metadata": {
    "tags": []
   },
   "source": [
    "**For this tutorial:** GOES (16, 17, 18) buckets: https://microsoft.github.io/AIforEarthDataSets/data/goes-r.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1b6d3-4013-4164-9228-95fce06f74a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from azure.storage.blob import ContainerClient\n",
    "import netCDF4\n",
    "\n",
    "# Select your desired variables by looking at the product documentation (see instructions above!)\n",
    "satellite = \"noaa-goes16\"\n",
    "product = \"ABI-L2-LSTM\"\n",
    "year = \"2023\"\n",
    "day_of_year = \"004\"\n",
    "hour = \"17\"\n",
    "\n",
    "name_string = f'{product}/{year}/{day_of_year}/{hour}/'\n",
    "\n",
    "# Using name_string as a file path, search for all files that match your criteria.\n",
    "container_client = ContainerClient(account_url='https://goeseuwest.blob.core.windows.net', \n",
    "                                                     container_name=satellite, credential=None)\n",
    "\n",
    "# See which files your search produced, and return the file name.\n",
    "fnames = []\n",
    "for f in container_client.walk_blobs(name_starts_with=name_string, delimiter='/'):\n",
    "    fnames.append(f.name)\n",
    "    print(f.name)\n",
    "           \n",
    "# Use the requests library to find each file that met your search criteria. \n",
    "for file in fnames:\n",
    "    resp = requests.get(f'https://goeseuwest.blob.core.windows.net/{satellite}/{file}')\n",
    "    \n",
    "    # Read the file in as a netCDF file, and you may begin analysis.\n",
    "    nc = netCDF4.Dataset(file, memory = resp.content)\n",
    "    \n",
    "    # your analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d799f8-985d-4d93-91cf-a542366525ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Google"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1baeb8-d471-4fa0-a26b-a43357ea7593",
   "metadata": {},
   "source": [
    "How to find datasets on Google:     \n",
    "1. Go to the NODD page (https://www.noaa.gov/nodd/datasets), locate your dataset, and click on the \"Google\" link.   \n",
    "2. From the Google product page, click the blue \"VIEW DATASET\" button. From here, you may select a project and discover your desired file path.\n",
    "3. Adjust the name_string variable in the cell below to reflect your new product path. You can either choose generic variable names, then fill them in with your specific request (as shown below), or copy the specific file path directly into name_string, and delete the generic variable definitions (i.e. name_string = \"ABI-L2-ACHAC/2023/003/02/\").      \n",
    "4. Be sure to change the bucket name (in this example, the bucket name is stored in the \"satellite\" variable) to reflect your product choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd57d98-25ca-4b3e-bef5-702e5c513384",
   "metadata": {
    "tags": []
   },
   "source": [
    "**For this tutorial:** GOES (16, 17, 18) buckets: https://console.cloud.google.com/marketplace/product/noaa-public/goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00d0f79-d6bb-430f-8825-e8743b22ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from google.cloud import storage\n",
    "import netCDF4\n",
    "\n",
    "# Select your desired variables by looking at the product documentation (see instructions above!)\n",
    "satellite = \"gcp-public-data-goes-16\"\n",
    "product = \"ABI-L2-LSTM\"\n",
    "year = \"2023\"\n",
    "day_of_year = \"004\"\n",
    "hour = \"17\"\n",
    "\n",
    "name_string = f'{product}/{year}/{day_of_year}/{hour}/'\n",
    "\n",
    "# Using name_string as a file path, search for all files that match your criteria.\n",
    "client = storage.Client.create_anonymous_client()\n",
    "bucket = client.bucket(bucket_name = satellite)\n",
    "\n",
    "blobs = bucket.list_blobs(prefix = name_string, delimiter = \"/\")\n",
    "response = blobs._get_next_page_response()\n",
    "\n",
    "# See which files your search produced, and return the file name and url.\n",
    "fnames = []\n",
    "urls = []\n",
    "\n",
    "for f in range(0, len(response[\"items\"])):\n",
    "    url_link = ((response[\"items\"])[f])[\"mediaLink\"]\n",
    "    urls.append(url_link)\n",
    "    fname = ((response[\"items\"])[f])[\"name\"]\n",
    "    fnames.append(fname)\n",
    "    \n",
    "    print(fname)\n",
    "    \n",
    "# Use the requests library to find each file that met your search criteria. \n",
    "for i in range(0,len(fnames)):\n",
    "    resp = requests.get(urls[i])\n",
    "    \n",
    "    # Read the file in as a netCDF file, and you may begin analysis.\\\n",
    "    nc = netCDF4.Dataset(fnames[i], memory = resp.content)\n",
    "    \n",
    "    # your analysis here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
